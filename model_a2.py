"""
A2: Physics-Informed Direct Mapping
Extends A1 by:
• Training on AC-OPF labels (imported from A1 generator)
• Using Neuromancer PenaltyLoss during training
• Optional PF projection after inference
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandapower as pp
import pandapower.networks as pn

# === Import A1 components ===
from model_a1 import DirectMap, generate_dummy_dataset_opf, train_direct

# === Neuromancer ===
from neuromancer.loss import PenaltyLoss, Constraint


# --------------------------------------------------------
# Physics penalty comparators
# --------------------------------------------------------

def gen_limit_comparator(y_pred, limits):
    lo, hi = limits
    low_violation  = torch.relu(limits[0] - y_pred) * 3.0 #higher penalty for negative violations
    high_violation = torch.relu(y_pred - limits[1]) * 1.0
    loss = (low_violation + high_violation).mean(dim=1, keepdim=True)
    return loss, loss.detach(), loss


def build_penalty_loss(net, n_gen):
    """Build generator & voltage limit penalties for A2."""

    # Use real generator active power limits from network
    pmin = torch.tensor(net.gen.min_p_mw.values, dtype=torch.float32)
    pmax = torch.tensor(net.gen.max_p_mw.values, dtype=torch.float32)
    p_limits = (pmin, pmax)

    # Use real voltage limits from network
    vmin = torch.tensor(net.bus.min_vm_pu.values[net.gen.bus.values], dtype=torch.float32)
    vmax = torch.tensor(net.bus.max_vm_pu.values[net.gen.bus.values], dtype=torch.float32)
    v_limits = (vmin, vmax)


    dummy_left  = torch.zeros((1, n_gen))   # required placeholder
    dummy_right = torch.zeros((1, n_gen))   # required placeholder

    constraints = [
        Constraint(
            left=dummy_left,
            right=dummy_right,
            comparator=lambda pred, ref: gen_limit_comparator(pred, p_limits),
            name="p_gen_limits"
        ),
        Constraint(
            left=dummy_left,
            right=dummy_right,
            comparator=lambda pred, ref: gen_limit_comparator(pred, v_limits),
            name="v_gen_limits"
        ),
    ]

    return PenaltyLoss(objectives=[], constraints=constraints)



# --------------------------------------------------------
# A2 physics-informed training loop
# --------------------------------------------------------

def train_a2(model, X, Y, epochs=40, lr=1e-3, lam=1.0):
    
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    mse = nn.MSELoss()

    n_gen = Y.shape[1] // 2
    penalty = build_penalty_loss(net, n_gen)

    for ep in range(1, epochs+1):
        opt.zero_grad()
        pred = model(X)

        p_pred = pred[:, :n_gen]
        v_pred = pred[:, n_gen:]

        # ----- NEW: power balance penalty -----
        total_load = torch.sum(X[:, :n_gen], dim=1, keepdim=True)  # approx active load sum
        total_gen  = torch.sum(p_pred, dim=1, keepdim=True)
        loss_balance = torch.mean((total_gen - total_load)**2)
        # --------------------------------------


        loss_mse  = mse(pred, Y)
        loss_phys = penalty({"p_gen_limits": p_pred,
                             "v_gen_limits": v_pred})["loss"]

        loss = loss_mse + lam * loss_phys + 0.001 * loss_balance
        loss.backward()
        opt.step()

        print(f"[A2] Ep {ep}/{epochs} | MSE={loss_mse.item():.4e} | Phys={loss_phys.item():.4e} | Total={loss.item():.4e}")

    return model


# --------------------------------------------------------
# PF feasibility projection
# --------------------------------------------------------

def pf_projection(net, x_load, p_pred, v_pred):
    """Non-differentiable PF layer for inference."""
    n_load = len(net.load)

    for i, idx in enumerate(net.load.index):
        net.load.at[idx, 'p_mw']   = float(x_load[i])
        net.load.at[idx, 'q_mvar'] = float(x_load[i+n_load])

    for j, g in enumerate(net.gen.index):
        net.gen.at[g, 'p_mw'] = float(p_pred[j])
        net.gen.at[g, 'vm_pu'] = float(v_pred[j])

    try:
        pp.runpp(net)
    except Exception as e:
        print("  [PF] Loadflow did not converge for this sample. Skipping. Error:", e)
        return None, None

    return net.res_gen.p_mw.values, net.res_gen.vm_pu.values[:len(net.gen)]


# --------------------------------------------------------
# Main execution
# --------------------------------------------------------

if __name__ == "__main__":
    net = pn.case14()


    print("\n=== Loading AC-OPF dataset generated by A1 ===")
    data = torch.load("acopf_dataset.pt")
    X_all = data["X"]
    Y_all = data["Y"]

    # simple split
    X_train = X_all[:60]
    Y_train = Y_all[:60]

    n_gen = Y_train.shape[1] // 2
    #Y_train[:, n_gen:] = torch.clamp(Y_train[:, n_gen:], 0.9, 1.1)

    X_test  = X_all[60:70]
    Y_test  = Y_all[60:70]

    n_in  = X_train.shape[1]
    n_out = Y_train.shape[1]

    # --- Train A1 ---
    print("\n=== Loading pretrained A1 model ===")
    model_a1 = DirectMap(n_in, n_out)
    model_a1.load_state_dict(torch.load("a1_model.pt"))


    # --- Train A2 ---
    print("\n=== Training A2 (physics-informed) ===")
    model_a2 = train_a2(model_a1, X_train, Y_train, epochs=30, lam=0.001)

    # --- Evaluate with PF feasibility ---
    print("\n=== Evaluating A2 with PF projection ===")
    n_gen = len(net.gen)
    with torch.no_grad():
        pred = model_a2(X_test)

    # Get physical limits from the network for clamping
    pmin = net.gen.min_p_mw.values
    pmax = net.gen.max_p_mw.values
    vmin = net.bus.min_vm_pu.values[net.gen.bus.values]
    vmax = net.bus.max_vm_pu.values[net.gen.bus.values]

    for i in range(len(X_test)):
        x = X_test[i].numpy()
        p_pred = pred[i, :n_gen].numpy()
        v_pred = pred[i, n_gen:].numpy()

        # ---- Clamp to physical limits before PF ----
        p_pred = np.clip(p_pred, pmin, pmax)
        v_pred = np.clip(v_pred, vmin, vmax)
        # --------------------------------------------

        p_pf, v_pf = pf_projection(net, x, p_pred, v_pred)
        if p_pf is None:
            print(f"\nSample {i}: PF did not converge, skipping comparison.")
            continue

        print(f"\nSample {i}:")
        print("Pg_pred:", np.round(p_pred, 3))
        print("Pg_PF  :", np.round(p_pf, 3))
        print("Vm_pred:", np.round(v_pred, 3))
        print("Vm_PF  :", np.round(v_pf, 3))

